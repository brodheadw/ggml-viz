# GGMLÂ Visualizer

> **A crossâ€‘platform, realâ€‘time dashboard that lets you *****see***** whatÊ¼s happening inside any GGMLâ€‘based runtime â€”Â from **`llama.cpp`** on a RaspberryÂ Pi to **`whisper.cpp`** on an M3Â Max.**

---

## 1Â â€¢Â Why does this exist?

Lowâ€‘level LLM runtimes like **GGML** squeeze every last drop of performance out of CPUs and GPUs, but they are effectively a black box once the code is running.  Developers currently debug with `printf()` and perf logs â€”Â painful and timeâ€‘consuming. **GGMLÂ Visualizer** removes that friction:

- **Graph view**Â â€“ live, interactive DOTâ€‘style compute graph with perâ€‘op timings.
- **Timeline view**Â â€“ flameâ€‘chart showing kernel launches, thread utilisation, cache misses, and memory transfers (similar to Tracy but domainâ€‘aware).
- **Tensor inspector**Â â€“ peek at activations, histograms, min/max, sparsity, quantisation buckets â€¦ while the model is still running.
- **Attention heatâ€‘map**Â â€“ for transformer models, display tokenâ€‘byâ€‘token attention scores in real time.
- **MemoryÂ arena explorer**Â â€“ visualise GGMLÊ¼s bumpâ€‘allocator, fragmentation, and live/peak usage.

If youÊ¼ve ever wondered **â€œwhy did my 70â€‘B model drop to 1Â tok/s after the 2â€‘kÂ context mark?â€** this tool gives you answers instantly.

---

## 2Â â€¢Â Features at a glance

| Category          | Feature                                            | Status    |
| ----------------- | -------------------------------------------------- | --------- |
| **Graph**         | Realâ€‘time compute graph (hierarchical + clustered) | âœ…Â Ready   |
|                   | Static graph import (`ggml_graph_dump_dot`)        | âœ…Â Ready   |
| **Timeline**      | CPU & GPU kernel flameâ€‘chart                       | ðŸ› Â Beta   |
| **Tensors**       | Onâ€‘hover statistics (mean/Ïƒ, sparsity)             | ðŸ› Â Beta   |
|                   | Slice & heatâ€‘map viewer                            | âŒÂ Planned |
| **Memory**        | Live arena visual + peak tracker                   | ðŸ› Â Beta   |
| **Modelâ€‘aware**   | Transformer attention & KVâ€‘cache heatâ€‘maps         | âŒÂ Planned |
| **Extensibility** | Plugin SDK (C++)                                   | ðŸ› Â Beta   |

Legend: âœ…Â = productionâ€‘ready Â· ðŸ› Â = usable but polishing Â· âŒÂ = stub / not started

---

## 3Â â€¢Â QuickÂ start (90Â seconds)

### 3.1Â Install prerequisites

```bash
# Ubuntu / Debian
sudo apt update && sudo apt install -y git cmake build-essential libgl1-mesa-dev libxinerama-dev libxcursor-dev libxi-dev libxrandr-dev

# macOSÂ (AppleÂ Silicon & Intel)
brew install cmake glfw
```

### 3.2Â Build

```bash
git clone --recursive https://github.com/yourâ€‘org/ggmlâ€‘visualizer.git
cd ggmlâ€‘visualizer
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)
```

### 3.3Â Run against llama.cpp

```bash
# inside build/
./bin/ggmlâ€‘viz â€‘â€‘llamaâ€‘exe /path/to/llama.cpp/main â€‘â€‘model /path/to/model.gguf \
             â€‘â€‘prompt "Hello,â€¯world!"
```

The dashboard opens automatically.  If youÊ¼d rather use Whisper:

```bash
./bin/ggmlâ€‘viz â€‘â€‘whisperâ€‘exe /path/to/whisper.cpp/main â€‘â€‘wav speech.wav
```

---

## 4Â â€¢Â Architecture

```mermaid
graph TD
  subgraph GGML Runtime
    GG[ggml.c] -->|calls| Hook[Instrumentation hooks]
  end
  Hook --> IPC{{Zeroâ€‘Copy IPC}}
  IPC -->|shared structs| ServerCore([
    Dataâ€‘Collector
    â€¢ ringâ€‘bufferÂ events\n    â€¢ tensor snapshots
  ])
  ServerCore --> API{{gRPCÂ / WebSocket}}
  API --> UI[Electron/ImGui frontâ€‘end]
```

- **Instrumentation hooks** â€“Â small patch (\~200Â LOC) to GGML that triggers a callback before/after each op; can be upstreamed.
- **Zeroâ€‘Copy IPC** â€“Â POSIX shared memory on Unix, `CreateFileMapping` on Windows.
- **Frontâ€‘end** â€“Â Desktop ImGui build or optional Electron client for web dashboards.

---

## 5Â â€¢Â SupportedÂ platforms & backâ€‘ends

| OS / Arch                  | CPU (AVX2 / AVXâ€‘512 / NEON) | GPU (Metal / CUDA / Vulkan) | Status |
| -------------------------- | --------------------------- | --------------------------- | ------ |
| macOS 12+ (arm64, x86\_64) | âœ”ï¸Ž                          | MetalÂ 2                     | âœ…      |
| Linux (x86\_64)            | âœ”ï¸Ž                          | CUDAÂ 11+, Vulkan            | âœ…      |
| WindowsÂ 10+                | âœ”ï¸Ž                          | CUDAÂ 11+, Vulkan (dxc)      | ðŸ›      |
| RaspberryÂ PiÂ 5             | âœ”ï¸Ž (NEON)                   | â€”                           | ðŸ›      |

Performance overhead is <Â 3â€¯% when the visualizer is detached and \~5â€¯% with live UI, measured on an M3Â Max (7â€‘B LLaMAâ€‘3, 4â€‘k context).

---

## 6Â â€¢Â Roadmap (2025â€‘Q3)

- **0.2.0** â€“ Full CPU timeline, tensor heatâ€‘maps, KVâ€‘cache view âœ¨
- **0.3.0** â€“ GPU kernel correlation (Metal & CUDA), quantâ€‘bucket viewer
- **0.4.0** â€“ Plugin SDK v1 + Python bindings
- **0.5.0** â€“ Attention & router head visualizer, export to SVG/JSON

See [`docs/CHANGELOG.md`](docs/CHANGELOG.md) for granular history.

---

## 7Â â€¢Â Contributing

1. **Pick an issue** tagged `goodâ€‘firstâ€‘issue` or `helpâ€‘wanted`.
2. Fork â†’ feature branch â†’ PR. Run `./scripts/lint.sh` before pushing.
3. Each PR must pass CI (clangâ€‘tidy, unit tests, sanitizers).
4. Sign the lightweight contributor agreement (in `docs/CLA.md`).

We especially welcome:

- **UI/UX polishers** (ImGui, DearÂ ImGuiÂ Docking, Electron)
- **GPU devs** â€“ Metal shaders & CUDA kernel tracing
- **Model folk** â€“ attention/KVâ€‘cache interpretation modules

---

## 8Â â€¢Â Index
To run tests:
- `mkdir -p build && cd build // create dir and navigate to build folder`
  `cmake .. DCMAKE_BUILD_TYPE=Debug -DGGML_METAL=OFF`
  `make -4`
  `./bin/test_some_program`

## 9Â â€¢Â License

`ggmlâ€‘visualizer` is licensed under the **ApacheÂ 2.0** license.  We use icons licensed under CCâ€‘BYâ€‘4.0; see `docs/THIRD_PARTY.md`.

---

## 10Â â€¢Â Credits & Inspiration

- GeorgiÂ Gerganov and the **GGML** community for the blazingâ€‘fast runtime.
- AnthropicÊ¼s **Neuronpedia** and MetaÊ¼s **LLM Transparency Tool** for paving the way in model interpretability.
- **Tracy** profiler for showing that realâ€‘time, lowâ€‘overhead visualisation is possible in C++.

*â€œThe best debugger is a graphical one you can keep open while your model runs.â€* â€“Â Someone on Discord

---
